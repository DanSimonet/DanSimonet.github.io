---
title: "Analyses"
output: html_document
---

The Tidyverse is a collection of R packages that adhere to the tidy data principles of data analysis and graphing. The purpose of these packages is to make working with data more efficient. The core Tidyverse packages were created by Hadley Wickham, but over the last few years other individuals have added some packages to the collective, which has significantly expanded our data analytical capabilities through improved ease of use and efficiency.

Once data is placed into the appropriate format, you can draw upon hundreds of packages to carry out various analyses. We will cover a few related to Psychology, including *factor analysis* and *scale scoring* in the `psych` package, ANOVA in the `afex` package, and regression using the built-in `lm()` function. 

For everything to run in the next set of sections, make sure you have installed and loaded the `tidyverse`, `psych`, `tidyr`, and `afex`.  

# `Dplyr`



# `Psych` and `Corrr`

First thing you may want to do is a exploratory factor analysis to identify whether your scales are hanging together. We will also incorporate some `dplyr` syntax to simplify the process. Let's use the built-in `bfi` package containing Big Five personality data to demonstrate some useful functions. 

```{r}
library(psych)
library(tidyverse)
head(bfi)
```

First, `psych` has some useful shortcuts for descriptive statistics. While I tend to prefer using `tidyverse` in combination with `mutate`, `group_by` and `summarize`, the `describe` and `describeBy` functions have similiar capabilities. 

```{r}
describe(bfi[,1:5])
describeBy(bfi[,1:5], group = bfi$gender)  # Compare agreeable item responses across genders
```

A second common step is to run a correlation matrix on your data to get a sense of variable associations. `psych` offers a few useful functions, such as `lowerCor()` to return rounded values for the lower diagonal of a matrix and `pairs.panels()` to visualize scatterplots for whole data frames. However, I prefer the `corrr` package because it is tidy (hence works with tidyverse), offers better visualizations, and has more functionality for working with correlation matrices. Demonstrations of both are provided below for just the agreeableness items in `bfi`.

```{r}
# Psych functions
lowerCor(bfi[,1:5])
pairs.panels(bfi[,1:5])

# Demonstrate functions for corrr
library(corrr)

d <- correlate(bfi[,1:5])           # Start with correlate which returns matrix as a tibble (amenable to dplyr)
d                                   # Note differences fro psych display
d %>% shave() %>% fashion()         # remove upper right triangle and print in pretty fashion
d %>% shave() %>% stretch() %>% 
  na.omit() %>% filter(r > .50)   # remove upper triangle, stretch into long format while omitting missing, and then find strongest correlations

d %>% shave() %>% rplot           # plot correlations according to size and direction
network_plot(d)                   # network plot of correlations
```

`Psych` places special emphasis on classical and modern approaches to psychometrics. Indeed, it was primarily built as a psychometrics package. See the documentation to learn about all its capabilities. One common goal in psychometrics as an attempt either to describe (components) or to explain (factors) the relationships between many observed variables in terms of a more limited set of components or latent factors.Two functions I particularly like are `fa.parallel` and `fa`. `fa.parallel` runs parallel factor techniques which compare observed eigen values of a correlation matrix with those of random data. `fa` uns exploratory factor analysis in the sense you need not specify which items load onto which factors; however, you *must* indicate the expected number of factors with the `nfactor` argument (default is 1). There are a variety of estimation methods, with the default being minimum residual factor analysis (can be changed with the `fm` function). Illustrations with the entire big five dataset are provided below.  

```{r}
fa.parallel(bfi[, 1:25])         # parallel analysis on entire big five item set

bfi_f <- fa(bfi[,1:25], nfactors = 5, rotate = "oblimin", fm = "ml") # efa on bfi items, extracting 5 factors using maximum likelihood
print(bfi_f, sort = TRUE)                                            # present factor loadings from highest to lowest
```

Finally, you can use the `score` function of `psych` to score items, run item analyses, and return reliability estimates. To score items on particular scales, we must creat a set of **scoring** keys. These simply tell us which items go on which scales. Let's quickly look at the scoring key for the first ten items of the Big Five scale by printing out `bfi.dictionary`.

```{r}
head(bfi.dictionary, 10)
```

We can see agreeableness has 1 negatively keyed item whereas conscientiousness has 2 negatively keyed items. The most intuitive way to create a scoring key is to create a **list** where each element is a scale containing a vector of keys. This vector can either be the actual names or an integer of their location. YOU also want to insert a `-` symbol in front of any character or integer to reverse an item. Here is an example for the first ten items of the BFI.

```{r}
keys.list <- list(Agreeable = c(-1, 2:5), Consc = c(6:8, -9, -10))  #creating scoring key based upon item location, noting where to reverse items based on the bfi.dictionary keying
keys.list2 <- list(Agreeable = c("-A1", "A2", "A3", "A4", "A5"), Consc = c("C1", "C2", "C3", "-C4", "-C4"))  #creating scoring key based upon item location, noting where to reverse items based on the bfi.dictionary keying

scores <-  scoreItems(keys.list, bfi)    # providing scoring key followed by data frame
scores    # print to investigate descriptive statistics, alpha, scale intercorrelations, and also get scale scores
```

We can see the internal consistency for agreeableness (.70) and conscientiousness (.72) are adequate, the average item correlations are on the lower but typical end for personality scales (.32 to .34), and the two scales have only a modest correlation of .26 suggesting adequate divergent validity. `scores` also contains the actual scores which you can easily extract and add into your data frame.

```{r}
head(scores$scores)
bfi_ac <- cbind(bfi, scores$scores)
```

## Exercise with Psych

1. Run a parallel and exploratory factor analysis on the 20 PANAS items from T1 in the NAPS data. Note you will need to eliminate missing data. 

```{r eval = F, error = T}
a <- NAPS %>% dplyr::select(T1_PANAS1:T1_PANAS20) %>% na.omit  # Eliminate NA and select only PANAS items
fa(a)
```
There appears to be a problem with T1_PANAS8. Let's explore closer. 

```{r eval = F, error = T}
glimpse(a)
cor(a)
```

Identified PANAS_8 as having zero variance, thus throwing an error. Eliminate from analysis. 

```{r eval = F, error = T}
library(psych)

a <- NAPS %>% dplyr::select(T1_PANAS1:T1_PANAS20) %>% na.omit 
ab <- a %>% dplyr::select(-T1_PANAS8)
```

Run a parallel analysis to determine number of factors to extract.

```{r eval = F, error = T}
fa.parallel(ab)
```

Run of the mill factor analysis, pulling out 2 factors given theory and parallel analysis. Used varimax by default. 

```{r eval = F, error = T}
fac <- fa(ab, nfactors = 2, rotate = "varimax")   # fa to run exploratory factor analysis
print(fac, sort=TRUE)                             # Sort loadings
```

2. Next, use the `score` function to automatically run item analyses, reliability, and calculate your scale scores for just the positive affect PANAS items. Add the scores to your dataset. The items are as follows: `T1_PANAS1`, `T1_PANAS3`, `T1_PANAS5`, `T1_PANAS9`, `T1_PANAS10`, `T1_PANAS12`, `T1_PANAS14`, `T1_PANAS16`, `T1_PANAS17`, `T1_PANAS19`


```{r eval = F, error = T}
keys.list <- list(PA = c("T1_PANAS1", "T1_PANAS3", "T1_PANAS5", "T1_PANAS9", "T1_PANAS10", "T1_PANAS12", "T1_PANAS14", "T1_PANAS16", 
                         "T1_PANAS17", "T1_PANAS19"))
PAscore <- scoreItems(keys.list, NAPS)
NAPS <- cbind(NAPS, PAscore$scores)
```

# Tidy Data

Tidy data is data that’s easy to work with: it’s easy to munge (with dplyr), visualise (with ggplot2 or ggvis) and model (with R’s hundreds of modelling packages). In order to facilitate the data analysis pipeline, it is crucial to have tidy data. What this means is that *every column in your data frame represents a variable and every row represents an observation.* This is also referred to as **long format** (as opposed to wide format).

`Gather` takes a **wide** data set and makes it **long**. This occurs when our data is "unstacked", meaning a common attribute or variable of concern is spread out across columns. In Psychology, the time of measurement is often spread across multiple columns with the same outcome being measured at each point. Other examples include within-person factors, splitting data by a group (e.g., male aggression, female aggression as separate columns), or measures from different people using the same tool (e.g., mother's depression, father's depression). Here are the main arguments for `gather`.

| Argument| Description| 
|:------------|:-------------------------------------------------|
|`data`| A data frame|
|`key`| Name of new column which will store the condition/IV columns which are spread out across your data |
|`value`| Name of new column which stores the response/DV values in the cells. |
|`...`| A selection of columns to be collapsed into the `key` column. Select a range with `:` or exclude columns with `-`. If blank, all columns are collapsed. |

Keys specify the name of a **new column** in which you will **gather up** several pre-existing colum names and assemble together. Values will also create a **new column**, but one which represents whatever the data in the cells represent. You can think of keys as corresponding to the columns of a variable which is too wide and the values as representing the same kind of information stored under each of these columns. Let's look at a few examples. Below I will simulate fake data where participants were asked to rate three different faces on attractiveness on a scale of 1 to 10. 

```{r}
set.seed(1)
wide <- data.frame(
  ID = c(1:10),
  Face.1 = round(rnorm(10, 7, 1), 0),
  Face.2 = round(rnorm(10, 5, 1), 0),
  Face.3 = round(rnorm(10, 3, 1), 0)
)
wide
```

This dataset is messy. As you can see, only ID is stored in a single column. Attractiveness rating (the DV) is split between the three faces, such that responses are in both rows **and** columns. What we want instead is one column for condition (Face.1, Face.2, or Face.3) responses and a column for attractiveness ratings, with each row being a *singular observation for each participant*. Participants IDs should repeat as this is a within-subect design (each participant saw each face). We can use `gather` to fix this issue.

```{r}
long <- gather(wide, Face, Attractive, Face.1:Face.3)
```

Let's take another example. Say we are doing a health experiment involving excercise (condition 2) or no activity (condition 1) as a between-subject IV and administering two types of drugs (a and b) as a within-person IV. At each drug administration we measure the participant's heart rate. Hypothetical data is presented below.

```{r}
messy <- data.frame(
  name = c("Wilbur", "Petunia", "Gregory", "Joe"),
  sex = c("m", "f", "m", "m"),
  condition = rep(1:2, each = 2),
  druga = c(67, 80, 64, 101),
  drugb = c(56, 90, 50, 66)
)
messy
```

What would we want to `gather` up inside of this data frame? In other words, where is there an attribute being split across columns with the same information store below? In this scenario, `drug` is the attribute we can gather up into its own column whereas `heart rate` is the corresponding value being measured. We can fix this as follows.

```{r}
tidied <- gather(messy, drug, heartrate, c(druga, drugb))
tidied
```

Sometimes two variables or more variables are clumped together in one column. The `separate()` function allows you to tease them apart and typically requires at least 4 arguments. 

| Argument| Description| 
|:------------|:-------------------------------------------------|
|`data`| A data frame|
|`col`| Name of the column you wish to split apart |
|`into`| Names of new variables you want to split the `col` into. |
|`sep`| The separator between attributes in the `col` argument. If a character (e.g., "a", "_", etc...), is interpreted as a regular expression. The default is a reg expression matching any sequence. Ifnumberic, interpreted as position to split (e.g., 4 characters in). POsitive values start at 1 position on left, negative values start -1 at at far right. |

Take a hypothetical study on work-family distraction. We are measuring how much people get distracted by their phone at both work and home at two time points. Participants are assigned to a mindfulness intervention (treatment) or nothing (control). 

```{r}
set.seed(10)
messy <- data.frame(
  id = 1:4,
  trt = sample(rep(c('control', 'treatment'), each = 2)),
  work.T1 = runif(4),
  home.T1 = runif(4),
  work.T2 = runif(4),
  home.T2 = runif(4)
)
messy
```

In this dataset, we would first want to `gather` the last 4 columns into a `key` column and the cell phone distraction into a `value` column called time. Once in long format, we then then `separate` the location (work v home) from the time point (T1 v T2) based upon the `.` which always delimits the two attributes. We will need to escape the period with two forward slashes, `\\`, so R does not interpret it as a special regex symbol. Here is how this could be specified. 

```{r}
tidier <- gather(messy, key, time, -id, -trt)      # Easier to say do NOT collapse id and treatment. R will understand to collapse all remaining columns.
tidier

tidy <- tidier %>%
  separate(key, into = c("Location", "Time Point"), sep = "\\.")
tidy
```

Finally, `spread()` is the complement of `gather()`. It takes long data frames and *spreads it out* to make it wide. Sometimes this is useful if exporting data to different programs, if a function requires wide (although this is rare in R), or just for presentation sake. The spread arguments are as follows.

| Argument| Description| 
|:------------|:-------------------------------------------------|
|`data`| A data frame|
|`key`| Name of column whose values will be used as column headings. |
|`value`| Names of column whos values will populate the cells. |
|

Remember our `long` dataframe from above with the faces. Let's see if we can change this back to its original wide format. In this scenario, we want the different Faces in the `Face` column to become new columns, one per face, while the actual dependent variable of attractiveness should populate these new face columns. In effect, this command is the reversal of what we did with `gather`. 

```{r}
spread(long, Face, Attractive)
```

And there you have it. You have come full circle back to wide. 

### Exercise - Tidying Up

1. There is an untidy dataset called `iris` built into R. The four columns represent the separate measurement of length and width of sepals and petals on three different flowers. All measures are made in centimeters. Can you tidy this data into just 3 columns?

2. Tidy the NAPS dataset based upon the 4 VAS columns. Once tidied, `separate` the columns into Time and Vas. 


```{r}
# Solution 1
head(iris)
gather(iris, PlantMeasure, Centimeters, -Species)    #Fastest way is to tell gather to assemble all columns except last one
```

```{r eval = F, error = T}
# Solution 2
NAPS_t <- gather(NAPS, Key, Value, c("T1_Vas1", "T1_Vas2", "T2_Vas1", "T2_Vas2")) # go from wide to long

NAPS_tidy <- separate(NAPS_t, Key, into = c("Time", "Vas"), sep = "_")
```

# Afex

Base R packages for ANOVA, especially for unbalanced designs or anything with repeated measures, is very convoluted. Note most statisticians and Big Data scientists rarely, if ever, use ANOVA-like analyses for their models. Rather, most use regression, multilevel models, or more advanced generalized linear models which are more flexible and make fewer assumptions than ANOVA. In many ways, ANOVA is a thing of the past. 

Yet, it still has its place and use. This is especially the case in highly controlled experiments or situations with many discrete variables. This need turned many people away from R becuse the analysis of such designs was nasty for many reasons. 

1. There was no easy syntax for within-subjects ANOVA
2. There were many differences in default settings compared to other software such as SPSS (Type III Sums of Squares)
3. The setting of appropriate contrasts

The package `afex` (analysis of factorial experiments), mainly written by [Henrik Singmann](http://singmann.org/), eases the process substantially. For all kinds of AN(C)OVA designs (between, within, mixed), you basically need only one function. In addition, it is recommended you install and load the package `emmeans` which perform all manner of post-hoc comparison and contrasts as well as `multcomp` which allows for more advanved forms of control for multiple tests. 

```{r}
library(afex)        # Need for ANOVA functions
library(emmeans)     # emmeans loaded for post-hoc pairwise tests and contrasts
library(multcomp)    # for advanced control of Type I errors in multiple tests
library(car)         # for obk.long dataset
```

For this example, we will use the `obk.long` data set (available in the `car` package) as well as the `fhch2010` example available from `afex` by typing `data("fhch2010")`. Let's first get familiar with the data set (always an important step).

```{r}
data("obk.long")
?obk.long
str(obk.long)
```

We can see in the description this is an imaginary study in which 16 female and male subjects are divided into three treatments (control, A, and B) and measured at a pre-test, post-test, and follow-up session. Further, within each session, they are measured at five occassions at intervals of one hour, hence hour is nested in phase. Before progressing, one thing to note is the `phase` has the factors ordered alphabetically as opposed to by the time they occurred (should be pre, post, fup as opposed to the other way around). If you remember from our factor lesson, we can modify these values as such:

```{r}
obk.long$phase <- factor(obk.long$phase, levels = c("pre", "post", "fup"))
str(obk.long)
```

We can see they have successfully been reordered. Continuing, the study design has two between-subject factors (gender, treatment) as well as two within-subject (phase, hour). The table below shows a breakdown of how the data are collected.

```{r}
with(obk.long, table(treatment, id, phase))
```

Since the data are made up, let's make this easier to visualize by imposing a realistic study. Let's assume the DV is amount learned and we are testing two educational interventions (a, b) to see if they improve learning over time (phases) compared to a control (control). 

## ANOVA

The first step is running an ANOVA analysis. Note afex provides *three* ways to call this function: `aov_ez`, `aov_car`, and `aov_4`. They all produce the same results but vary in the formula conventions used to by different R packages. For our purposes, we will focus first on the `aov_car` to understand R's basic ANOVA syntax and then show `aov_ez` which is more human readable way to provide your code. 

When specifying ANOVA (and pretty much any other formula) in R, you must give an equation representing your model in the form of `Y ~ X1 + X2...` where `Y` is your DV, `X1` is your first predictor, `X2` is your second predictor, and so forth. To represent interactions between variables, you must place a `:` between them and add them into the equation like this: `Y ~ X1 + X2 + X1:X2`. A simpler way to have R automatically tally all possible interactions between variables is to separate them with an `*` like this: `Y ~ X1 * X2`. Both forms will return the same results. Once specified, you must also include a `data = data frame name` wherein you specify the name of the data frame containing your variables. 

Within `aov_car`, you also must add an `Error` term to the formula specifying the column containing the participant (or unit of observation) identifier (e.g., minimally `+ Error(id)`). When a within- or repeated-measures factor is included, you must divide the error term by the within-subject error as follows: `Error(id/repeated-measure)`. Turning back to our `obk.long` data, let's look at the effects of `treatment` and `phase` on `value`.

```{r}
# Specify the anova formula and store output as object

a <- aov_car(value ~ treatment + Error(id/phase), data = obk.long)   # formula specification
a <- aov_car(value ~ treatment + Error(id/phase), data = obk.long)   # Same result. The Bet * With interaction term is not necessary.
                                                                     # I often do for full transparency.
a
```

The printed output is an ANOVA table that could basically be copied into a manusript as is. One sees the terms in the column `Effect`, the degrees of freedom (`df`), the mean-squared error (`MSE` - might remove for a paper), the **F**-value along with a significance notation, and the **p**-value. The unfamiliar `ges` is a generalized eta-square, which is the recommended effect size for repeated measure designs. If you run `summary(a)` you will get further information, including Machly's test for sphericity and GG as well as HF corrections. Note we can use the `afex_aov` objects to print nice tables with the `knitr` package.

```{r}
knitr::kable(nice(a))
```

In terms of results, we have a significant effect for phase which indicates learning likely improves over time (practice effects). However, there is no main effect for treatment (p = .09). But, this is qualified by a significant treatment by phase interaction, which requires further probing. 

## Post-Hoc Contrasts



## Basic Plots

The recent `afex` version comes with the `afex_plot` function which plots estimated marginal means, error bars, and raw data using `ggplot2` whih allows further modifications. Error bars can be based on different standard errors (e.g., model-based, within-subjects, between-subjects). You need to specify `x` factors, which determine which factor-levels or combinations of factor-levels are plotted on the x-axis. YOu can also defined `trace` factor(s) which determines which factor levels are connected by lines. Finally, there is the optional `panel` factor(s) for splitting the plot into subplots. `afex_plot` then plots the estimated marginal means obtained from `emmeans`, confidence intervals, and the raw data in the background. Note that the raw data in the background is per default drawn using an alpha blending of .5 (i.e., 50% semi-transparency). Thus, in case of several points lying directly on top of each other, this point appears noticeably darker.

```{r}
afex_plot(a, x = "phase", trace = "treatment") + theme_classic()
```



# Regression

# Tables
